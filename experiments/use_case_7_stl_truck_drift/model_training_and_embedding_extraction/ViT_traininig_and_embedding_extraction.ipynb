{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a64398-b8b3-489f-ac2e-3ec6c82dcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Load images from a pickle file\n",
    "with open(\"stl_test.pickle\", \"rb\") as tr_images_file:\n",
    "    test = pickle.load(tr_images_file)\n",
    "with open(\"stl_train.pickle\", \"rb\") as tr_images_file:\n",
    "    train = pickle.load(tr_images_file)\n",
    "with open(\"stl_val.pickle\", \"rb\") as tr_images_file:\n",
    "    new = pickle.load(tr_images_file)\n",
    "with open(\"stl_deg.pickle\", \"rb\") as tr_images_file:\n",
    "    deg = pickle.load(tr_images_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d5f2284-8e2b-4c46-afbc-c663402dae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eccezioni\n",
      "0\n",
      "eccezioni\n",
      "0\n",
      "eccezioni\n",
      "0\n",
      "eccezioni\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image as im \n",
    "\n",
    "def unpk(data):\n",
    "\n",
    "    x=[]\n",
    "    y=[]\n",
    "    c=0\n",
    "    tot=[]\n",
    "    for pair in data:\n",
    "        try:\n",
    "            img=np.array(pair[0]).reshape((96,96,3))\n",
    "            x.append(img)\n",
    "            y.append(pair[1])\n",
    "        except Exception as e:\n",
    "            c=c+1\n",
    "    \n",
    "    print(\"exceptions\")\n",
    "    print(c)\n",
    "    return x, y\n",
    "\n",
    "x_train,y_trai=unpk(train)\n",
    "x_test,y_tes=unpk(test)\n",
    "x_new, y_ne=unpk(new)\n",
    "x_deg, y_deg=unpk(deg)\n",
    "c2=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "674d2975-fa1f-4f19-b235-6c806e9c736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_deg=[]\n",
    "y_train=[]\n",
    "y_test=[]\n",
    "y_new=[]\n",
    "for i in range(len(x_deg)):\n",
    "    y_deg.append(9)\n",
    "\n",
    "for i in range(len(y_trai)):\n",
    "    if y_trai[i]==9:\n",
    "        y_train.append(3)\n",
    "    else:\n",
    "        y_train.append(y_trai[i])\n",
    "\n",
    "for i in range(len(y_tes)):\n",
    "    if y_tes[i]==9:\n",
    "        y_test.append(3)\n",
    "    else:\n",
    "        y_test.append(y_tes[i])\n",
    "\n",
    "for i in range(len(y_ne)):\n",
    "    if y_ne[i]==9:\n",
    "        y_new.append(3)\n",
    "    else:\n",
    "        y_new.append(y_ne[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3d62ce4-9fcf-4564-8b45-385fa4238dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_x=x_train.copy()\n",
    "new_train_y=y_train.copy()\n",
    "\n",
    "new_x_test=[]\n",
    "new_y_test=[]\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "        new_x_test.append(x_test[i])\n",
    "        new_y_test.append(y_test[i])\n",
    "    \n",
    "\n",
    "new_train_x=np.array(new_train_x)\n",
    "new_test_x=np.array(new_x_test)\n",
    "new_train_y=np.array(new_train_y)\n",
    "new_test_y=np.array(new_y_test)\n",
    "x_new=np.array(x_new)\n",
    "x_deg=np.array(x_deg)\n",
    "y_new=np.array(y_new)\n",
    "y_deg=np.array(y_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b000d56-0482-4fa0-a60b-068990957182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "5850\n",
      "test\n",
      "2925\n",
      "new\n",
      "2925\n",
      "drift\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "def create_ds(im, lab):\n",
    "    ds=[]\n",
    "    for i in range(len(im)):\n",
    "        ds.append((im[i],lab[i]))\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "drift=create_ds(x_deg, y_deg)\n",
    "train_ds=create_ds(new_train_x, new_train_y)\n",
    "test_ds=create_ds(new_test_x, new_test_y)\n",
    "new_ds=create_ds(x_new, y_new)\n",
    "\n",
    "print(\"train\")   \n",
    "print(len(train_ds))\n",
    "print(\"test\")\n",
    "print(len(test_ds))\n",
    "print(\"new\")\n",
    "print(len(new_ds))\n",
    "print(\"drift\")\n",
    "print(len(drift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1502e0ca-3cc2-4a70-aaf5-176b8443528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=9):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "          return logits, loss.item()\n",
    "        else:\n",
    "          return logits, None\n",
    "    \n",
    "    def emb_extr_new_v1(self, pixel_values):\n",
    "        \n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        #output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        #for layer in model.children():\n",
    "        \n",
    "        return outputs.last_hidden_state[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9890e5eb-29d0-445d-82e8-e8b6462e5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 2e-5\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0230b537-1ee5-430a-8a39-9aa34cf6f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# Define Model\n",
    "model = ViTForImageClassification(9)    \n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Use GPU if available  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e67dad7-4bb5-4a89-b159-4a59e0be337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inizio training...\n",
      "Epoch:  0 | train loss: 2.1914 | test accuracy: 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.25      0.22         4\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.02        50\n",
      "   macro avg       0.02      0.03      0.02        50\n",
      "weighted avg       0.02      0.02      0.02        50\n",
      "\n",
      "[[1 0 0 2 0 1 0 0 0]\n",
      " [0 0 0 0 0 6 0 0 0]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 0 0 0 2 0 3]\n",
      " [0 1 0 0 0 0 2 1 1]\n",
      " [1 1 4 1 0 0 0 0 0]\n",
      " [0 4 0 1 0 1 0 3 0]\n",
      " [0 0 2 0 0 1 0 0 0]\n",
      " [1 0 2 1 0 1 0 0 0]]\n",
      "Epoch:  0 | train loss: 1.5808 | test accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "[[ 2  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  6]]\n",
      "Epoch:  0 | train loss: 0.8731 | test accuracy: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      0.83      0.91         6\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.96        50\n",
      "   macro avg       0.98      0.93      0.94        50\n",
      "weighted avg       0.97      0.96      0.96        50\n",
      "\n",
      "[[8 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 7 0 0 0 0 0]\n",
      " [1 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 8 0 0 0]\n",
      " [0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 0 0 7 0]\n",
      " [0 0 0 0 0 0 0 0 4]]\n",
      "Epoch:  1 | train loss: 0.7621 | test accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         6\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       0.80      1.00      0.89         4\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.98      0.96      0.97        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n",
      "[[4 0 0 0 0 0 0 0 0]\n",
      " [0 6 0 0 0 0 0 0 0]\n",
      " [0 0 6 0 0 0 0 0 0]\n",
      " [0 0 0 6 0 0 0 0 0]\n",
      " [0 0 0 0 2 1 0 0 0]\n",
      " [0 0 0 0 0 4 0 0 0]\n",
      " [0 0 0 0 0 0 7 0 0]\n",
      " [0 0 0 0 0 0 0 8 0]\n",
      " [0 0 0 0 0 0 0 0 6]]\n",
      "Epoch:  1 | train loss: 0.5421 | test accuracy: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       1.00      0.89      0.94         9\n",
      "           2       1.00      0.78      0.88         9\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       0.80      1.00      0.89         4\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.94        50\n",
      "   macro avg       0.93      0.96      0.94        50\n",
      "weighted avg       0.95      0.94      0.94        50\n",
      "\n",
      "[[4 0 0 0 0 0 0 0 0]\n",
      " [0 8 0 0 0 1 0 0 0]\n",
      " [1 0 7 0 0 0 0 0 1]\n",
      " [0 0 0 5 0 0 0 0 0]\n",
      " [0 0 0 0 8 0 0 0 0]\n",
      " [0 0 0 0 0 4 0 0 0]\n",
      " [0 0 0 0 0 0 4 0 0]\n",
      " [0 0 0 0 0 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 4]]\n",
      "Epoch:  1 | train loss: 0.3437 | test accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91         6\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.86      1.00      0.92         6\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       1.00      1.00      1.00         6\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.98      0.98      0.98        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n",
      "[[5 0 1 0 0 0 0 0 0]\n",
      " [0 7 0 0 0 0 0 0 0]\n",
      " [0 0 6 0 0 0 0 0 0]\n",
      " [0 0 0 4 0 0 0 0 0]\n",
      " [0 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0]\n",
      " [0 0 0 0 0 0 6 0 0]\n",
      " [0 0 0 0 0 0 0 8 0]\n",
      " [0 0 0 0 0 0 0 0 3]]\n",
      "Epoch:  2 | train loss: 0.3345 | test accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.86      1.00      0.92         6\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      0.90      0.95        10\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.98      0.99      0.99        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n",
      "[[6 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0]\n",
      " [0 0 6 0 0 0 0 0 0]\n",
      " [0 0 0 6 0 0 0 0 0]\n",
      " [0 0 1 0 9 0 0 0 0]\n",
      " [0 0 0 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 4 0 0]\n",
      " [0 0 0 0 0 0 0 6 0]\n",
      " [0 0 0 0 0 0 0 0 5]]\n",
      "Epoch:  2 | train loss: 0.2462 | test accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         9\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         8\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "[[7 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 0 7 0 0 0 0 0 0]\n",
      " [0 0 0 9 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 8 0 0 0]\n",
      " [0 0 0 0 0 0 4 0 0]\n",
      " [0 0 0 0 0 0 0 8 0]\n",
      " [0 0 0 0 0 0 0 0 3]]\n",
      "Epoch:  2 | train loss: 0.2222 | test accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "           3       1.00      0.80      0.89         5\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.94      0.98      0.95        50\n",
      "weighted avg       0.99      0.98      0.98        50\n",
      "\n",
      "[[ 1  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  0]\n",
      " [ 1  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  2]]\n",
      "Epoch:  3 | train loss: 0.1799 | test accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "[[5 0 0 0 0 0 0 0 0]\n",
      " [0 9 0 0 0 0 0 0 0]\n",
      " [0 0 4 0 0 0 0 0 0]\n",
      " [0 0 0 7 0 0 0 0 0]\n",
      " [0 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0]\n",
      " [0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 0 0 8 0]\n",
      " [0 0 0 0 0 0 0 0 2]]\n",
      "Epoch:  3 | train loss: 0.1585 | test accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         6\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "[[3 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0]\n",
      " [0 0 6 0 0 0 0 0 0]\n",
      " [0 0 0 8 0 0 0 0 0]\n",
      " [0 0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 0 6 0 0 0]\n",
      " [0 0 0 0 0 0 7 0 0]\n",
      " [0 0 0 0 0 0 0 8 0]\n",
      " [0 0 0 0 0 0 0 0 5]]\n",
      "Epoch:  3 | train loss: 0.1438 | test accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         6\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         6\n",
      "           7       1.00      1.00      1.00        11\n",
      "           8       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "[[ 4  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  0  0  0  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  6]]\n",
      "Epoch:  4 | train loss: 0.1890 | test accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89         5\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       0.92      1.00      0.96        11\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       1.00      1.00      1.00         4\n",
      "           8       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.99      0.98      0.98        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n",
      "[[ 4  0  1  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  0]\n",
      " [ 0  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  4  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  5]]\n",
      "Epoch:  4 | train loss: 0.2364 | test accuracy: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      0.83      0.91         6\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      0.83      0.91         6\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.60      1.00      0.75         3\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.96        50\n",
      "   macro avg       0.96      0.96      0.95        50\n",
      "weighted avg       0.98      0.96      0.96        50\n",
      "\n",
      "[[11  0  0  0  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  1  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  0  1  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  5]]\n",
      "Epoch:  4 | train loss: 0.1890 | test accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      0.80      0.89         5\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.97      0.98      0.97        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n",
      "[[ 4  0  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0]\n",
      " [ 0  0  0  4  0  0  0  0  1]\n",
      " [ 0  0  0  0  3  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  3]]\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n",
    "print(\"training...\")\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):        \n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        # Change input array into list with each batch being one element\n",
    "        x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
    "        # Remove unecessary dimension\n",
    "        for index, array in enumerate(x):\n",
    "            x[index] = np.squeeze(array)\n",
    "        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
    "        x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "        # Send to GPU if available\n",
    "        x, y  = x.to(device), y.to(device)\n",
    "        b_x = Variable(x)   # batch x (image)\n",
    "        b_y = Variable(y)   # batch y (target)\n",
    "        # Feed through model\n",
    "        output, loss = model(b_x, None)\n",
    "        # Calculate loss\n",
    "        if loss is None: \n",
    "            loss = loss_func(output, b_y)   \n",
    "            optimizer.zero_grad()           \n",
    "            loss.backward()                 \n",
    "            optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            # Get the next batch for testing purposes\n",
    "            test = next(iter(test_loader))\n",
    "            test_x = test[0]\n",
    "            # Reshape and get feature matrices as needed\n",
    "            test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
    "            for index, array in enumerate(test_x):\n",
    "                test_x[index] = np.squeeze(array)\n",
    "            test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
    "            # Send to appropriate computing device\n",
    "            test_x = test_x.to(device)\n",
    "            test_y = test[1].to(device)\n",
    "            # Get output (+ respective class) and compare to target\n",
    "            test_output, loss = model(test_x, test_y)\n",
    "            test_output = test_output.argmax(1)\n",
    "            # Calculate Accuracy\n",
    "            accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be019b43-11f3-46ae-b044-1bdbbc34d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_stl10_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7240e18c-243c-4a53-ba71-2b4706a225f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inizio evaluation...\n",
      "[[317   0   2   5   0   1   0   0   0]\n",
      " [  0 318   0   0   0   3   0   3   1]\n",
      " [ 24   0 279   6   4   1   3   1   7]\n",
      " [  6   0   0 316   1   0   0   0   2]\n",
      " [  7   0   1   1 308   1   7   0   0]\n",
      " [  0   0   0   0   0 317   0   8   0]\n",
      " [  6   0   0   2   2   1 314   0   0]\n",
      " [  0   0   0   0   0   2   0 323   0]\n",
      " [  1   0   1   8   0   0   1   0 314]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.92       325\n",
      "           1       1.00      0.98      0.99       325\n",
      "           2       0.99      0.86      0.92       325\n",
      "           3       0.93      0.97      0.95       325\n",
      "           4       0.98      0.95      0.96       325\n",
      "           5       0.97      0.98      0.97       325\n",
      "           6       0.97      0.97      0.97       325\n",
      "           7       0.96      0.99      0.98       325\n",
      "           8       0.97      0.97      0.97       325\n",
      "\n",
      "    accuracy                           0.96      2925\n",
      "   macro avg       0.96      0.96      0.96      2925\n",
      "weighted avg       0.96      0.96      0.96      2925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "num_classes = 9  # Number of classes\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define a custom dataset class for the test set\n",
    "class CustomTestDataset():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "        # Perform any necessary preprocessing on the image\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        image = transform(Image.fromarray(image))\n",
    "        return image, label\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataset = CustomTestDataset(test_ds)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "# Perform inference on the test set\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "print(\"evaluation...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs, loss = model(inputs, labels)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        #all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75e0ba25-d781-4d5a-8a1f-d5a9a9be697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "inizio extraction embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2519450/3601715191.py:14: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  embeddings=np.array(embeddings, dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "inizio extraction embedding...\n",
      "new\n",
      "inizio extraction embedding...\n",
      "deg\n",
      "inizio extraction embedding...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def embedding_extraction(modello, dataloader):\n",
    "    \n",
    "    embeddings=[]\n",
    "    print(\"extraction embedding...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "\n",
    "        for images, _ in dataloader:\n",
    "            outputs = modello.emb_extr_new_v1(images)\n",
    "            embeddings.append(outputs)\n",
    "            \n",
    "    embeddings=np.array(embeddings, dtype='object')\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "def into_dataloader(ds):\n",
    "    dataset = CustomTestDataset(ds)\n",
    "    dataloader = data.DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"train\")\n",
    "train_emb=embedding_extraction(model, into_dataloader(train_ds))\n",
    "print(\"test\")\n",
    "test_emb=embedding_extraction(model, into_dataloader(test_ds))\n",
    "print(\"new\")\n",
    "new_emb=embedding_extraction(model, into_dataloader(new))\n",
    "print(\"drift\")\n",
    "deg_emb=embedding_extraction(model, into_dataloader(deg))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecad075f-4003-4ddf-967c-026866509b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions...\n",
      "train\n",
      "inizio evaluation...\n",
      "test\n",
      "inizio evaluation...\n",
      "new\n",
      "inizio evaluation...\n",
      "deg\n",
      "inizio evaluation...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def prediction_extraction(modello, dataloader):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs, loss = modello(inputs, labels=None)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            #all_embeddings.append(embeddings.cpu().numpy())\n",
    "            \n",
    "    return all_predictions, all_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"predictions...\")\n",
    "print(\"train\")\n",
    "train_pred, train_lab=prediction_extraction(model, into_dataloader(train_ds))\n",
    "print(\"test\")\n",
    "test_pred, test_lab=prediction_extraction(model, into_dataloader(test_ds))\n",
    "print(\"new\")\n",
    "new_pred, new_lab=prediction_extraction(model, into_dataloader(new_ds))\n",
    "print(\"drift\")\n",
    "deg_pred, deg_lab=prediction_extraction(model, into_dataloader(drift))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d8e6528-8eb0-4ae9-a7ab-0f3cf42218b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debatching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2519450/1429373657.py:7: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  final_e=np.array(final_e, dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def de_batch(emb):\n",
    "    final_e=[]\n",
    "    for i in range(len(emb)):\n",
    "        for j in range(len(emb[i])):\n",
    "            final_e.append(emb[i][j])\n",
    "    \n",
    "    final_e=np.array(final_e, dtype='object')\n",
    "    return final_e\n",
    "\n",
    "print(\"debatching...\")\n",
    "\n",
    "final_train_emb=de_batch(train_emb)\n",
    "final_test_emb=de_batch(test_emb)\n",
    "final_new_emb=de_batch(new_emb)\n",
    "final_deg_emb=de_batch(deg_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8211b4ad-753c-4dea-96f6-5fbb2c3b666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "test...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "new...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "deg...\n",
      "0\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def final_step(dati):\n",
    "    temp=[]\n",
    "    for i in range(len(dati)):\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "        tmp=[]\n",
    "        for j in range(len(dati[i])):\n",
    "            tmp.append(dati[i][j])\n",
    "        temp.append(np.array(tmp))\n",
    "    \n",
    "    temp=np.array(temp)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "print(\"train...\")\n",
    "fin_train_emb=final_step(final_train_emb)\n",
    "print(\"test...\")\n",
    "fin_test_emb=final_step(final_test_emb)\n",
    "print(\"new...\")\n",
    "fin_new_emb=final_step(final_new_emb)\n",
    "print(\"drift...\")\n",
    "fin_deg_emb=final_step(final_deg_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f281f58d-8883-423a-92b4-4d646169d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings saved\n",
      "embeddings saved\n",
      "embeddings saved\n",
      "embeddings saved\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def save_embeddings(embedding, true_label,pr_label, path):\n",
    "    fp = h5py.File(path, \"w\")\n",
    "    fp.create_dataset(\"E\", data=embedding, compression=\"gzip\")    \n",
    "    fp.create_dataset(\"Y_predicted\", data=pr_label, compression=\"gzip\") \n",
    "    fp.create_dataset(\"Y_original\", data=true_label, compression=\"gzip\") \n",
    "    fp.close()\n",
    "    print(\"embeddings saved\")\n",
    "\n",
    "train_path=\"vit_Stl_train_emb.hdf5\"\n",
    "test_path=\"vit_Stl_test_emb.hdf5\"\n",
    "new_path=\"vit_Stl_new_emb.hdf5\"\n",
    "deg_path=\"vit_Stl_deg_emb.hdf5\"\n",
    "\n",
    "save_embeddings(fin_train_emb, train_lab, train_pred, train_path )\n",
    "save_embeddings(fin_test_emb, test_lab, test_pred, test_path )\n",
    "save_embeddings(fin_new_emb, new_lab, new_pred, new_path)\n",
    "save_embeddings(fin_deg_emb, deg_lab, deg_pred, deg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3adb1-c1aa-477a-9abc-3086c2754da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}