{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d4bc84e-921d-46b6-b225-91febb6a45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "train_df = pd.read_csv('mnist_train.csv')\n",
    "test_df = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "# Extract labels and features\n",
    "train_labels = train_df.iloc[:, 0]  # the label is in the first column\n",
    "train_features = train_df.iloc[:, 1:]  \n",
    "test_labels = test_df.iloc[:, 0]  # Assuming the label is in the first column\n",
    "test_features = test_df.iloc[:, 1:]  \n",
    "\n",
    "# Convert labels and features into numpy arrays\n",
    "train_labels_array = train_labels.to_numpy()\n",
    "train_features_array = train_features.to_numpy()\n",
    "test_labels_array = test_labels.to_numpy()\n",
    "test_features_array = test_features.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b852ddf2-98b3-4bf4-811a-4e0ce3701ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "<class 'numpy.ndarray'>\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels_array))\n",
    "print(len(test_labels_array))\n",
    "print(type(train_features_array))\n",
    "\n",
    "print(len(train_features_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461ddc06-5dde-48aa-a6a0-62733a6a99c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing label 9 from train...\n",
      "removing label 9 from test...\n",
      "13783\n",
      "48200\n",
      "8017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deg_l1=9\n",
    "deg_l2=8\n",
    "deg_data=[]\n",
    "deg_labels=[]\n",
    "\n",
    "new_train_data=[]\n",
    "new_train_lab=[]\n",
    "print(\"removing labels 8 9 from train...\")\n",
    "for i in range(len(train_labels_array)):\n",
    "    if train_labels_array[i]==deg_l1 or train_labels_array[i]==deg_l2:\n",
    "        deg_data.append(train_features_array[i])\n",
    "        deg_labels.append(train_labels_array[i])\n",
    "    else:\n",
    "        new_train_data.append(train_features_array[i])\n",
    "        new_train_lab.append(train_labels_array[i])\n",
    "        \n",
    "new_test_data=[]\n",
    "new_test_lab=[]\n",
    "print(\"removing labels 8 9 from test...\")\n",
    "for i in range(len(test_labels_array)):\n",
    "    if test_labels_array[i]==deg_l1 or test_labels_array[i]==deg_l2:\n",
    "        deg_data.append(test_features_array[i])\n",
    "        deg_labels.append(test_labels_array[i])\n",
    "    else:\n",
    "        new_test_data.append(test_features_array[i])\n",
    "        new_test_lab.append(test_labels_array[i])\n",
    "        \n",
    "print(len(deg_data))\n",
    "print(len(new_train_data))\n",
    "print(len(new_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4ce810-8928-4ad9-a243-fb78f102cf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48200\n",
      "48200\n",
      "12532\n",
      "35668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create data splits\n",
    "rp=13/50\n",
    "print(len(new_train_data))\n",
    "print(len(new_train_lab))\n",
    "nn_tr_data,new_unseen_data, nn_tr_lab ,new_unseen_lab=train_test_split(new_train_data, new_train_lab, test_size=rp, stratify=new_train_lab, random_state=42)\n",
    "\n",
    "\n",
    "print(len(new_unseen_data))\n",
    "print(len(nn_tr_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aaa8fd0-53b2-4521-b2d0-708350b34484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "35000\n",
      "8000\n",
      "12000\n",
      "13000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Balance datasets\n",
    "\n",
    "final_train_data=[]\n",
    "final_train_lab=[]\n",
    "final_test_data=[]\n",
    "final_test_lab=[]\n",
    "final_unseen_data=[]\n",
    "final_unseen_lab=[]\n",
    "final_deg_data=[]\n",
    "final_deg_lab=[]\n",
    "deg_label=8\n",
    "c=0\n",
    "for i in range(35000):\n",
    "        final_train_data.append(nn_tr_data[i])\n",
    "        final_train_lab.append(nn_tr_lab[i])\n",
    "\n",
    "for i in range(12000):\n",
    "        final_unseen_data.append(new_unseen_data[i])\n",
    "        final_unseen_lab.append(new_unseen_lab[i])    \n",
    "        \n",
    "for i in range(8000):\n",
    "        final_test_data.append(new_test_data[i])\n",
    "        final_test_lab.append(new_test_lab[i]) \n",
    "\n",
    "c=50\n",
    "for i in range(13000):\n",
    "    try:\n",
    "        final_deg_data.append(deg_data[i])\n",
    "        final_deg_lab.append(deg_label)\n",
    "    except Exception as e:\n",
    "        final_deg_data.append(deg_data[i-c])\n",
    "        final_deg_lab.append(deg_label)\n",
    "        c=c+1\n",
    "\n",
    "print(len(final_train_lab))\n",
    "print(len(final_train_data))\n",
    "print(len(final_test_lab))\n",
    "print(len(final_unseen_lab))\n",
    "print(len(final_deg_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c784a045-8b22-4452-92b6-2efe8b3472cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "exceptions\n",
      "0\n",
      "exceptions\n",
      "0\n",
      "exceptions\n",
      "0\n",
      "exceptions\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def image_conv(imgs_array):\n",
    "\n",
    "    x=[]\n",
    "    c=0\n",
    "    for i in range(len(imgs_array)):\n",
    "        try:\n",
    "            rgb_image = np.stack((imgs_array[i],) * 3, axis=-1)\n",
    "            img=np.array(rgb_image).reshape((28,28,3))\n",
    "            x.append(img)\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"An exception has occurred\")\n",
    "\n",
    "    return x\n",
    "\n",
    "# Store the csv values in numpy arrays\n",
    "print(final_train_data[0].shape)\n",
    "ult_train_data=image_conv(final_train_data)\n",
    "ult_test_data=image_conv(final_test_data)\n",
    "ult_unseen_data=image_conv(final_unseen_data)\n",
    "ult_deg_data=image_conv(final_deg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb936887-08ff-416c-8673-2069d0d5b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Define the model\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=8):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "            return logits, loss.item()\n",
    "        else:\n",
    "            return logits, None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def emb_extr_new_v1(self, pixel_values):\n",
    "        \n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        \n",
    "        return outputs.last_hidden_state[:,0]\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb77f0d-9ea4-4ce8-9e9c-304a0985558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n",
      "unseen\n",
      "deg\n"
     ]
    }
   ],
   "source": [
    "# Transform the data from numpy array to images\n",
    "def data_into_format(data, labels):\n",
    "    formatted_data=[]\n",
    "    for i in range(len(data)):\n",
    "        formatted_data.append((data[i][0], labels[i]))\n",
    "\n",
    "    \n",
    "    return formatted_data\n",
    " \n",
    "# Convert images into tensors\n",
    "transform_2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to fit the input size of the transformer\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize images\n",
    "])\n",
    "\n",
    "print(\"Train data\")\n",
    "f_train_data=[]\n",
    "for i in range(len(ult_train_data)):\n",
    "    img_pil = Image.fromarray(np.uint8(ult_train_data[i]))\n",
    "    f_train_data.append(img_pil)\n",
    "\n",
    "print(\"Test data\")\n",
    "f_test_data=[]\n",
    "for i in range(len(ult_test_data)):\n",
    "    img_pil = Image.fromarray(np.uint8(ult_test_data[i]))\n",
    "    f_test_data.append(img_pil)\n",
    "\n",
    "print(\"New unseen data\")\n",
    "f_unseen_data=[]\n",
    "for i in range(len(ult_unseen_data)):\n",
    "    img_pil = Image.fromarray(np.uint8(ult_unseen_data[i]))\n",
    "    f_unseen_data.append(img_pil)\n",
    "\n",
    "\n",
    "print(\"Drifted data\")\n",
    "f_deg_data=[]\n",
    "for i in range(len(ult_deg_data)):\n",
    "    img_pil = Image.fromarray(np.uint8(ult_deg_data[i]))\n",
    "    f_deg_data.append(img_pil)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170c17fc-ce61-49da-8b47-23817ce7e7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7e3cb61ab34aa0b0d1864f9f9cc7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/68.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8489e5dba6914e24ae46c41b693e17d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "\n",
    "#BATCH_SIZE = 100\n",
    "EPOCHS=3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model=ViTForImageClassification()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5a9e77-06d0-42d3-8bae-529424709b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, batch_size, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = (idx + 1) * self.batch_size\n",
    "        \n",
    "        batch_images = self.images[start_idx:end_idx]\n",
    "        batch_labels = self.labels[start_idx:end_idx]\n",
    "        \n",
    "        # Check if batch is empty\n",
    "        if not batch_images:\n",
    "            raise ValueError(\"Empty batch. Please check the dataset and batch size.\")\n",
    "        \n",
    "        if self.transform:\n",
    "            batch_images = [self.transform(image) for image in batch_images]\n",
    "        else:\n",
    "            batch_images = [transforms.ToTensor()(image) for image in batch_images]\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "        \n",
    "        return torch.stack(batch_images), batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "235f6e01-2400-4929-8c6c-43913efb4137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "training...\n",
      "0\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "Epoch [1/3], Loss: 0.1616\n",
      "Accuracy on training set: 95.33%\n",
      "1\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "Epoch [2/3], Loss: 0.0890\n",
      "Accuracy on training set: 96.29%\n",
      "2\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "Epoch [3/3], Loss: 0.0616\n",
      "Accuracy on training set: 96.90%\n",
      "Accuracy on test set: 97.75%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor epoch in range(EPOCHS):        \\n    for step, (x, y) in enumerate(train_loader):\\n        # Change input array into list with each batch being one element\\n        x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\\n        # Remove unecessary dimension\\n        for index, array in enumerate(x):\\n            x[index] = np.squeeze(array)\\n        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\\n        x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\\n        # Send to GPU if available\\n        x, y  = x.to(device), y.to(device)\\n        b_x = Variable(x)   # batch x (image)\\n        b_y = Variable(y)   # batch y (target)\\n        # Feed through model\\n        output, loss = model(b_x, None)\\n        # Calculate loss\\n        if loss is None: \\n            loss = loss_func(output, b_y)   \\n            optimizer.zero_grad()           \\n            loss.backward()                 \\n            optimizer.step()\\n\\n        if step % 10 == 0:\\n            # Get the next batch for testing purposes\\n            test = next(iter(test_loader))\\n            test_x = test[0]\\n            # Reshape and get feature matrices as needed\\n            test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)\\n            for index, array in enumerate(test_x):\\n                test_x[index] = np.squeeze(array)\\n            test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\\n            # Send to appropirate computing device\\n            test_x = test_x.to(device)\\n            test_y = test[1].to(device)\\n            # Get output (+ respective class) and compare to target\\n            test_output, loss = model(test_x, test_y)\\n            test_output = test_output.argmax(1)\\n            # Calculate Accuracy\\n            accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\\n            print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "train_loader= CustomDataset(f_train_data, final_train_lab, batch_size=batch_size, transform=transform_2)\n",
    "test_loader = CustomDataset(f_test_data, final_test_lab, batch_size=batch_size, transform=transform_2)\n",
    "\n",
    "print(\"Start training...\")\n",
    "\n",
    "correct=0\n",
    "total=0\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(train_loader)):\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "        \n",
    "        images=train_loader[i][0].to(device)\n",
    "        labels=train_loader[i][1].to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(images, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    print(f\"Accuracy on training set: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_loader)):\n",
    "        images=test_loader[i][0].to(device)\n",
    "        labels=test_loader[i][1].to(device)\n",
    "        outputs, _ = model(images, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc6a25d-7648-4c39-9024-7254ea4a1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'vit_mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6824bf40-5d4f-47de-a844-a84c3fe16edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training extraction...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "test extraction...\n",
      "0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Embedding extraction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def extract_embeddings(loader, model, device):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(loader)):\n",
    "            if i%100==0:\n",
    "                print(i)\n",
    "            images=loader[i][0].to(device)\n",
    "            labels=loader[i][1].to(device)\n",
    "            embeddings=model.emb_extr_new_v1(pixel_values=images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels)\n",
    "        \n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0) if all_embeddings else torch.tensor([])\n",
    "    all_labels = torch.cat(all_labels, dim=0) if all_labels else torch.tensor([])\n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "\n",
    "print(\"Training extraction...\")\n",
    "# Extract embeddings for training and test sets\n",
    "train_embeddings, train_labels = extract_embeddings(train_loader, model, device)\n",
    "print(\"Test extraction...\")\n",
    "test_embeddings, test_labels = extract_embeddings(test_loader, model, device)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "700f65f8-79d2-42f1-81c5-8c5954a0a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "new unseen extraction...\n",
      "0\n",
      "100\n",
      "deg extraction...\n",
      "0\n",
      "100\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load new unseen data and simulated drift data(label 8,9) in custom dataset\n",
    "\n",
    "new_loader= CustomDataset(f_unseen_data, final_unseen_lab, batch_size=batch_size, transform=transform_2)\n",
    "deg_loader = CustomDataset(f_deg_data, final_deg_lab, batch_size=batch_size, transform=transform_2)\n",
    "print(len(train_loader))\n",
    "\n",
    "\n",
    "print(\"New unseen extraction...\")\n",
    "# Extract embeddings for new unseen data and drift data\n",
    "new_embeddings, new_labels = extract_embeddings(new_loader, model, device)\n",
    "print(\"Drifted data extraction...\")\n",
    "deg_embeddings, deg_labels = extract_embeddings(deg_loader, model, device)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54c0b8b1-92ca-4a66-941f-24abddabacb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred extraction...\n",
      "35000\n",
      "test emb extraction...\n",
      "8000\n",
      "unseen emb extraction...\n",
      "12000\n",
      "deg emb extraction...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 8 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     44\u001B[0m unseen_pred\u001B[38;5;241m=\u001B[39mfinal_get_pred(model, new_loader)\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeg emb extraction...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 46\u001B[0m deg_pred\u001B[38;5;241m=\u001B[39m\u001B[43mfinal_get_pred\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeg_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03mprint(\"predictions...\")\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03mprint(\"train\")\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03mprint(\"done\")\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36mfinal_get_pred\u001B[0;34m(model, data_loader)\u001B[0m\n\u001B[1;32m     24\u001B[0m images\u001B[38;5;241m=\u001B[39mdata_loader[i][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     25\u001B[0m labels\u001B[38;5;241m=\u001B[39mdata_loader[i][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 26\u001B[0m outputs, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     28\u001B[0m xx\u001B[38;5;241m.\u001B[39mappend(predicted)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36mViTForImageClassification.forward\u001B[0;34m(self, pixel_values, labels)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     29\u001B[0m     loss_fct \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m---> 30\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits, loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/loss.py:1150\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1149\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1151\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1152\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2846\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   2844\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2845\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 2846\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: Target 8 is out of bounds."
     ]
    }
   ],
   "source": [
    "def prediction_extraction(modello, dataloader):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs, loss = modello(inputs, labels=None)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "    return all_predictions, all_labels\n",
    "\n",
    "def final_get_pred(model, data_loader):\n",
    "    \n",
    "    xx=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data_loader)):\n",
    "\n",
    "            images=data_loader[i][0].to(device)\n",
    "            labels=data_loader[i][1].to(device)\n",
    "            outputs, _ = model(images,labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            xx.append(predicted)\n",
    "\n",
    "    new_xx=[]\n",
    "    for i in range(len(xx)):\n",
    "        for j in range(len(xx[i])):\n",
    "            new_xx.append(xx[i][j])\n",
    "    print(len(new_xx))\n",
    "    \n",
    "    return new_xx\n",
    "\n",
    "# Extract the predictions of the model\n",
    "\n",
    "print(\"train data predicted label prediction...\")\n",
    "train_pred=final_get_pred(model, train_loader)\n",
    "print(\"test data predicted label prediction...\")\n",
    "test_pred=final_get_pred(model, test_loader)\n",
    "print(\"new unseen data predicted label prediction...\")\n",
    "unseen_pred=final_get_pred(model, new_loader)\n",
    "#print(\"drifted data predicted label prediction...\")\n",
    "#deg_pred=final_get_pred(model, deg_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d79d917d-0b9a-492c-bdb2-102eb71055cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "deg emb extraction...\n",
      "13000\n",
      "so far so good...\n"
     ]
    }
   ],
   "source": [
    "#since labels 8 and 9 are not available to the model we temporally\n",
    "#label them as 0, the model is in evaluation mode so it won't update itself\n",
    "\n",
    "\n",
    "temp_deg_labels=[]\n",
    "tmp=final_test_lab[0]\n",
    "print(type(tmp))\n",
    "for i in range(len(deg_labels)):\n",
    "    temp_deg_labels.append(0)\n",
    "\n",
    "new_deg_loader = CustomDataset(f_deg_data, temp_deg_labels, batch_size=batch_size, transform=transform_2)\n",
    "\n",
    "\n",
    "print(\"drifted data predicted label prediction......\")\n",
    "new_deg_pred=final_get_pred(model, new_deg_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3baf36ab-8dcf-4d2d-b668-74b83795bc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings saved\n",
      "embeddings saved\n",
      "embeddings saved\n",
      "embeddings saved\n",
      "the end\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def save_embeddings(embedding, true_label,pr_label, path):\n",
    "    fp = h5py.File(path, \"w\")\n",
    "    fp.create_dataset(\"E\", data=embedding, compression=\"gzip\")    \n",
    "    fp.create_dataset(\"Y_predicted\", data=pr_label, compression=\"gzip\") \n",
    "    fp.create_dataset(\"Y_original\", data=true_label, compression=\"gzip\") \n",
    "    fp.close()\n",
    "    print(\"embeddings saved\")\n",
    "\n",
    "train_path=\"vit_MNIST_train_emb.hdf5\"\n",
    "test_path=\"vit_MNIST_test_emb.hdf5\"\n",
    "new_path=\"vit_MNIST_new_emb.hdf5\"\n",
    "deg_path=\"vit_MNIST_deg_emb.hdf5\"\n",
    "\n",
    "save_embeddings(train_embeddings, final_train_lab, train_pred, train_path )\n",
    "save_embeddings(test_embeddings, final_test_lab, test_pred, test_path )\n",
    "save_embeddings(new_embeddings, final_unseen_lab, unseen_pred, new_path)\n",
    "save_embeddings(deg_embeddings, final_deg_lab, new_deg_pred, deg_path)\n",
    "print(\"the end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223513f-82b8-446e-9769-216c45d80e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_embeddings))\n",
    "print(len(test_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "194fbf69-bc99-42bb-9189-26c562aba7a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m CustomDataset(f_test_data, final_test_lab, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Assuming 'model' is an instance of your VisionTransformer model\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m train_embeddings, train_labels \u001B[38;5;241m=\u001B[39m \u001B[43mextract_transformer_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m test_embeddings, test_labels \u001B[38;5;241m=\u001B[39m extract_transformer_embeddings(model, test_dataset)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mso far so good...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36mextract_transformer_embeddings\u001B[0;34m(model, dataset)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m images, batch_labels \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;66;03m# Forward pass through the model to obtain transformer encoder outputs\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m         _, transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;66;03m# Append the transformer encoder outputs and corresponding labels\u001B[39;00m\n\u001B[1;32m     15\u001B[0m         embeddings\u001B[38;5;241m.\u001B[39mappend(transformer_outputs)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36mVisionTransformer.forward\u001B[0;34m(self, x, patch_dim)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m patch_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     33\u001B[0m     patch_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_dim  \u001B[38;5;66;03m# Use self.patch_dim if patch_dim is not provided\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m B, C, H, W \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m     35\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_embed(x)\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     36\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([torch\u001B[38;5;241m.\u001B[39mzeros(B, \u001B[38;5;241m1\u001B[39m, x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), x], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "def extract_transformer_embeddings(model, dataset):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, batch_labels in dataloader:\n",
    "            # Forward pass through the model to obtain transformer encoder outputs\n",
    "            _, transformer_outputs = model(images)\n",
    "\n",
    "            # Append the transformer encoder outputs and corresponding labels\n",
    "            embeddings.append(transformer_outputs)\n",
    "            labels.append(batch_labels)\n",
    "\n",
    "    # Concatenate embeddings and labels along the batch dimension\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return embeddings, labels\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(f_train_data, final_train_lab, batch_size=batch_size, transform=transform)\n",
    "test_dataset = CustomDataset(f_test_data, final_test_lab, batch_size=batch_size, transform=transform)\n",
    "\n",
    "# Assuming 'model' is an instance of your VisionTransformer model\n",
    "train_embeddings, train_labels = extract_transformer_embeddings(model, train_dataset)\n",
    "test_embeddings, test_labels = extract_transformer_embeddings(model, test_dataset)\n",
    "print(train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331398b9-5557-40fe-a33e-c22560f849ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1cbb7e-06c4-4f24-af26-6369ace8dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "def embedding_extraction(modello, dataloader):\n",
    "    \n",
    "    embeddings=[]\n",
    "    print(\"extraction embedding...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "\n",
    "        for images, _ in dataloader:  # Adjust this loop based on your data loading\n",
    "            outputs = modello.emb_extr_new_v1(images)\n",
    "            embeddings.append(outputs)\n",
    "            \n",
    "    embeddings=np.array(embeddings)\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "def extract_embeddings_from_dataset(dataset, model):\n",
    "    embeddings = []\n",
    "    for i in range(len(dataset)):\n",
    "        image, _ = dataset[i]\n",
    "        embeddings.append(model.emb_extr_new_v1(image))\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "\n",
    "def into_dataloader(ds):\n",
    "    dataloader = data.DataLoader(ds, batch_size=500, shuffle=False)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Extract embeddings from training and test data\n",
    "\n",
    "\n",
    "# Perform inference on the test set\n",
    "train_loader= CustomDataset(f_train_data, final_train_lab, batch_size=batch_size, transform=transform)\n",
    "test_loader = CustomDataset(f_test_data, final_test_lab, batch_size=batch_size, transform=transform)\n",
    "unseen_loader= CustomDataset(f_unseen_data, final_unseen_lab, batch_size=batch_size, transform=transform)\n",
    "deg_loader = CustomDataset(f_deg_data, final_deg_lab, batch_size=batch_size, transform=transform)\n",
    "print(\"train\")\n",
    "train_embeddings = extract_embeddings_from_dataset(train_loader, model)\n",
    "print(\"test\")\n",
    "test_embeddings = extract_embeddings_from_dataset(test_loader, model)\n",
    "print(\"new\")\n",
    "unseen_embeddings = extract_embeddings_from_dataset(unseen_loader, model)\n",
    "print(\"drift\")\n",
    "deg_embeddings = extract_embeddings_from_dataset(deg_loader, model)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9096e-1771-4afb-b603-f35395ec08f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4dd705-c8c4-41b2-9854-264f7621d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "num_classes = 8  # Replace with the number of classes in your specific task\n",
    "\n",
    "model.eval()\n",
    "\n",
    "class CustomTestDataset():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "        # Perform any necessary preprocessing on the image\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        image = transform(Image.fromarray(image))\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "#train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "#test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n",
    "#unseen_loader = data.DataLoader(new_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "#deg_loader  = data.DataLoader(deg_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n",
    "\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataset = CustomTestDataset(test_ds)\n",
    "print(test_dataset[0])\n",
    "test_dataloader = data.DataLoader(test_ds, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Perform inference on the test set\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "print(\"evaluation...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs, loss = model(inputs, labels)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        #all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "438d54a6-3ddc-4670-9718-62d7a3ebb42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "extraction embedding...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input image size (28*3) doesn't match model (224*224).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Perform inference on the test set\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 32\u001B[0m train_emb\u001B[38;5;241m=\u001B[39m\u001B[43membedding_extraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minto_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m test_emb\u001B[38;5;241m=\u001B[39membedding_extraction(model, into_dataloader(test_ds))\n",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36membedding_extraction\u001B[0;34m(modello, dataloader)\u001B[0m\n\u001B[1;32m      7\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, _ \u001B[38;5;129;01min\u001B[39;00m dataloader:  \u001B[38;5;66;03m# Adjust this loop based on your data loading\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodello\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memb_extr_new_v1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m#emb_extr_new_v1\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     embeddings\u001B[38;5;241m.\u001B[39mappend(outputs)\n",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36mViTForImageClassification.emb_extr_new_v1\u001B[0;34m(self, pixel_values)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21memb_extr_new_v1\u001B[39m(\u001B[38;5;28mself\u001B[39m, pixel_values):\n\u001B[0;32m---> 50\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixel_values\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;66;03m#output = self.dropout(outputs.last_hidden_state[:,0])\u001B[39;00m\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;66;03m#for layer in model.children():\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state[:,\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py:540\u001B[0m, in \u001B[0;36mViTModel.forward\u001B[0;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[0m\n\u001B[1;32m    533\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[1;32m    538\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m--> 540\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    542\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[1;32m    543\u001B[0m     embedding_output,\n\u001B[1;32m    544\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    547\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m    548\u001B[0m )\n\u001B[1;32m    549\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py:112\u001B[0m, in \u001B[0;36mViTEmbeddings.forward\u001B[0;34m(self, pixel_values, interpolate_pos_encoding)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, pixel_values, interpolate_pos_encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    111\u001B[0m     batch_size, num_channels, height, width \u001B[38;5;241m=\u001B[39m pixel_values\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m--> 112\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;66;03m# add the [CLS] token to the embedded patch tokens\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     cls_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token\u001B[38;5;241m.\u001B[39mexpand(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py:152\u001B[0m, in \u001B[0;36mPatchEmbeddings.forward\u001B[0;34m(self, pixel_values, interpolate_pos_encoding)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m interpolate_pos_encoding:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m height \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_size[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mor\u001B[39;00m width \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_size[\u001B[38;5;241m1\u001B[39m]:\n\u001B[0;32m--> 152\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    153\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput image size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mheight\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m*\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwidth\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match model (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_size[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m*\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_size[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    154\u001B[0m         )\n\u001B[1;32m    155\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprojection(pixel_values)\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[0;31mValueError\u001B[0m: Input image size (28*3) doesn't match model (224*224)."
     ]
    }
   ],
   "source": [
    "def embedding_extraction(modello, dataloader):\n",
    "    \n",
    "    embeddings=[]\n",
    "    print(\"extraction embedding...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "\n",
    "        for images, _ in dataloader:  # Adjust this loop based on your data loading\n",
    "            outputs = modello.emb_extr_new_v1(images)\n",
    "            embeddings.append(outputs)\n",
    "            \n",
    "    embeddings=np.array(embeddings)\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "def into_dataloader(ds):\n",
    "    dataloader = data.DataLoader(ds, batch_size=1000, shuffle=False)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "# Perform inference on the test set\n",
    "\n",
    "\n",
    "print(\"train\")\n",
    "train_emb=embedding_extraction(model, into_dataloader(train_ds))\n",
    "print(\"test\")\n",
    "test_emb=embedding_extraction(model, into_dataloader(test_ds))\n",
    "print(\"new\")\n",
    "new_emb=embedding_extraction(model, into_dataloader(new_ds))\n",
    "print(\"drift\")\n",
    "deg_emb=embedding_extraction(model, into_dataloader(deg_ds))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f85bc-19c2-4957-a732-968c477b0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_extraction(modello, dataloader):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"inizio evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs, loss = modello(inputs, labels=None)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "    return all_predictions, all_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"predictions...\")\n",
    "print(\"train\")\n",
    "train_pred, train_lab=prediction_extraction(model, into_dataloader(train_ds))\n",
    "print(\"test\")\n",
    "test_pred, test_lab=prediction_extraction(model, into_dataloader(test_ds))\n",
    "print(\"new\")\n",
    "new_pred, new_lab=prediction_extraction(model, into_dataloader(new_ds))\n",
    "print(\"drift\")\n",
    "deg_pred, deg_lab=prediction_extraction(model, into_dataloader(deg_ds))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af204bb-39f9-4a14-9916-4d16816fffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_emb))\n",
    "print(len(train_lab))\n",
    "\n",
    "print(len(train_emb[0]))\n",
    "print(train_lab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e050ba09-5df3-41bb-b278-055d015ac216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debatching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2138420/1921475119.py:7: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  final_e=np.array(final_e, dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def de_batch(emb):\n",
    "    final_e=[]\n",
    "    for i in range(len(emb)):\n",
    "        for j in range(len(emb[i])):\n",
    "            final_e.append(emb[i][j])\n",
    "    \n",
    "    final_e=np.array(final_e, dtype='object')\n",
    "    return final_e\n",
    "\n",
    "print(\"debatching...\")\n",
    "\n",
    "final_train_emb=de_batch(train_emb)\n",
    "final_test_emb=de_batch(test_emb)\n",
    "final_new_emb=de_batch(new_emb)\n",
    "final_deg_emb=de_batch(deg_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f843a05-3c06-4ff6-9def-c09647d95bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "test...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "new...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "deg...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def final_step(dati):\n",
    "    temp=[]\n",
    "    for i in range(len(dati)):\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "        tmp=[]\n",
    "        for j in range(len(dati[i])):\n",
    "            tmp.append(dati[i][j])\n",
    "        temp.append(np.array(tmp))\n",
    "    \n",
    "    temp=np.array(temp)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "print(\"train...\")\n",
    "fin_train_emb=final_step(final_train_emb)\n",
    "print(\"test...\")\n",
    "fin_test_emb=final_step(final_test_emb)\n",
    "print(\"new...\")\n",
    "fin_new_emb=final_step(final_new_emb)\n",
    "print(\"deg...\")\n",
    "fin_deg_emb=final_step(final_deg_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef46edc1-7c29-49c5-9483-3aa75fb70419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings saved\n",
      "embeddings saved\n",
      "embeddings saved\n",
      "embeddings saved\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def save_embeddings(embedding, true_label,pr_label, path):\n",
    "    fp = h5py.File(path, \"w\")\n",
    "    fp.create_dataset(\"E\", data=embedding, compression=\"gzip\")    \n",
    "    fp.create_dataset(\"Y_predicted\", data=pr_label, compression=\"gzip\") \n",
    "    fp.create_dataset(\"Y_original\", data=true_label, compression=\"gzip\") \n",
    "    fp.close()\n",
    "    print(\"embeddings saved\")\n",
    "\n",
    "train_path=\"vit_Intel_train_emb.hdf5\"\n",
    "test_path=\"vit_Intel_test_emb.hdf5\"\n",
    "new_path=\"vit_Intel_new_emb.hdf5\"\n",
    "deg_path=\"vit_Intel_deg_emb.hdf5\"\n",
    "\n",
    "save_embeddings(fin_train_emb, train_lab, train_pred, train_path )\n",
    "save_embeddings(fin_test_emb, test_lab, test_pred, test_path )\n",
    "save_embeddings(fin_new_emb, new_lab, new_pred, new_path)\n",
    "save_embeddings(fin_deg_emb, deg_lab, deg_pred, deg_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}