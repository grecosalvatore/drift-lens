{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2d0d0b-603f-4e56-a6b5-6ccc64591677",
   "metadata": {},
   "source": [
    "# Use Case 8: Gender Classification in Face Images - Embdding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a29bfe2-9bc5-43b4-88ca-377717a2d059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import datasets\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d397f-c456-41fb-a744-d583ab515c8b",
   "metadata": {},
   "source": [
    "## Load the Saved Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f529e29-999a-4b30-aa99-5a20a5ef3749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGE_CLASSES = [\"0-2\", \"3-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"more than 70\"]\n",
    "GENDER_CLASSES = [\"Male\", \"Female\"]\n",
    "RACE_CLASSES = [\"East Asian\", \"Indian\", \"Black\", \"White\", \"Middle Eastern\", \"Latino_Hispanic\", \"Southeast Asian\"]\n",
    "\n",
    "# Create dictionaries mapping class names to their index\n",
    "age2id = {age: idx for idx, age in enumerate(AGE_CLASSES)}\n",
    "gender2id = {gender: idx for idx, gender in enumerate(GENDER_CLASSES)}\n",
    "race2id = {race: idx for idx, race in enumerate(RACE_CLASSES)}\n",
    "\n",
    "# Create reverse mappings from index to class name\n",
    "id2age = {idx: age for age, idx in age2id.items()}\n",
    "id2gender = {idx: gender for gender, idx in gender2id.items()}\n",
    "id2race = {idx: race for race, idx in race2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba0c4c69-580d-47b1-8318-a0fab7f52392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = datasets.load_from_disk(\"fairface/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ec2ab4-ed16-4c89-a414-f0f62754aef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img_bytes', 'age', 'gender', 'race', 'id'],\n",
       "        num_rows: 44425\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img_bytes', 'age', 'gender', 'race', 'id'],\n",
       "        num_rows: 9438\n",
       "    })\n",
       "    drifted: Dataset({\n",
       "        features: ['img_bytes', 'age', 'gender', 'race', 'id'],\n",
       "        num_rows: 13835\n",
       "    })\n",
       "    new_unseen: Dataset({\n",
       "        features: ['img_bytes', 'age', 'gender', 'race', 'id'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5a7d821-d0f3-4da4-8fa2-6ee68f0a4ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bytes_to_pil(example_batch):\n",
    "    example_batch['img'] = [\n",
    "        Image.open(BytesIO(b)) for b in example_batch.pop('img_bytes')\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "ds = ds.with_transform(bytes_to_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2675a-195a-4b97-9ed7-ef48f66b7685",
   "metadata": {},
   "source": [
    "## Load the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07432d3-de87-4eb5-b1f9-a09abb7bd32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9544b303-6852-41bc-b690-4f649e0a3464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"fairface\", \"saved_model\", \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a877b0-b53b-4db4-9215-93eda6c1f9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load ViT model and image processor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,  # Gender classification (Male/Female)\n",
    "    id2label={0: \"Male\", 1: \"Female\"},\n",
    "    label2id={\"Male\": 0, \"Female\": 1},\n",
    "    ignore_mismatched_sizes=True,\n",
    "    output_hidden_states=True  # Ensure hidden states are returned\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6572e2de-7c17-4c79-8b55-e621601774fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to preprocess images before batching.\n",
    "    Converts images to pixel values using the ViT processor.\n",
    "    \"\"\"\n",
    "    images = [item[\"img\"] for item in batch]\n",
    "    ids = [item[\"id\"] for item in batch]\n",
    "    genders = [item[\"gender\"] for item in batch]\n",
    "    \n",
    "    # Convert images to tensors using the processor\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"id\": ids,\n",
    "        \"gender\": torch.tensor(genders, dtype=torch.long)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "706b703c-6b2a-4c92-9851-587f51e17db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_embedding_and_predict(model, processor, dataset, layer_id):\n",
    "    X = []\n",
    "    E = np.empty((0, 768))\n",
    "    Y_original = []\n",
    "    Y_original_names = []\n",
    "    Y_predicted = []\n",
    "    Y_predicted_names = []\n",
    "\n",
    "    BATCH_SIZE = 32  # Adjust as needed\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values, output_hidden_states=True)\n",
    "\n",
    "        batch_probabilities = torch.nn.functional.softmax(outputs[\"logits\"], dim=-1)\n",
    "        batch_labels = torch.argmax(batch_probabilities, dim=1).tolist()\n",
    "        batch_labels_name = [model.config.id2label[l] for l in batch_labels]\n",
    "\n",
    "        last_layer_hidden_states = outputs[\"hidden_states\"][layer_id]\n",
    "        embedding_CLS = last_layer_hidden_states[:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "        X.extend(batch[\"id\"])\n",
    "        E = np.vstack([E, embedding_CLS])\n",
    "        Y_original.extend(batch[\"gender\"].tolist())\n",
    "        Y_original_names.extend([model.config.id2label[l] for l in batch[\"gender\"].tolist()])\n",
    "        Y_predicted.extend(batch_labels)\n",
    "        Y_predicted_names.extend(batch_labels_name)\n",
    "\n",
    "    return X, E, Y_original, Y_original_names, Y_predicted, Y_predicted_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aead1ab1-259a-4af1-b7ed-987453b265f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_embedding(output_path, X, E, Y_original, Y_original_names, Y_predicted, Y_predicted_names):\n",
    "    with h5py.File(output_path, \"w\") as fp:\n",
    "        fp.create_dataset(\"X\", data=np.array(X, dtype=\"S\"), compression=\"gzip\")  # Convert to bytes for HDF5\n",
    "        fp.create_dataset(\"E\", data=E, compression=\"gzip\")\n",
    "        fp.create_dataset(\"Y_original\", data=np.array(Y_original, dtype=int), compression=\"gzip\")\n",
    "        fp.create_dataset(\"Y_original_names\", data=np.array(Y_original_names, dtype=\"S\"), compression=\"gzip\")\n",
    "        fp.create_dataset(\"Y_predicted\", data=np.array(Y_predicted, dtype=int), compression=\"gzip\")\n",
    "        fp.create_dataset(\"Y_predicted_names\", data=np.array(Y_predicted_names, dtype=\"S\"), compression=\"gzip\")\n",
    "    print(f\"Saved embeddings to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9152a175-d815-4fe6-a046-8221da04a571",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1389/1389 [05:31<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to fairface/saved_embedding/train.h5\n",
      "Processing test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:53<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to fairface/saved_embedding/test.h5\n",
      "Processing new_unseen split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [03:18<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to fairface/saved_embedding/new_unseen.h5\n",
      "Processing drifted split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [01:22<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to fairface/saved_embedding/drifted.h5\n"
     ]
    }
   ],
   "source": [
    "layer_id = -1  # Last layer\n",
    "\n",
    "for split in [\"train_embedding\", \"test_embedding\", \"new_unseen_embedding\", \"drift_embedding\"]:\n",
    "    print(f\"Processing {split} split...\")\n",
    "    X, E, Y_original, Y_original_names, Y_predicted, Y_predicted_names = extract_embedding_and_predict(\n",
    "        model, processor, ds[split], layer_id\n",
    "    )\n",
    "    \n",
    "    output_path = f\"fairface/saved_embedding/{split}.h5\"\n",
    "    save_embedding(output_path, X, E, Y_original, Y_original_names, Y_predicted, Y_predicted_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b31349-9bce-483b-a575-feb16575c2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb-XAI-env",
   "language": "python",
   "name": "airbnb-xai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
